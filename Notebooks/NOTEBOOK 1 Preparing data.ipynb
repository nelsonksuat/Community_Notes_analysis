{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4deea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi there, welcome to NOTEBOOK 1: Preparing data, in this notebook we will prepare our datasets for the next steps.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load a .tsv file into a DataFrame\n",
    "file_path = # Replace with your file path to 'Notes' dataset  \n",
    "df_notes_dataset = pd.read_csv(file_path, sep='\\\\t')\n",
    "\n",
    "\n",
    "# Remove columns by their indexes\n",
    "columns_to_remove = [1, 3, 5, 6, 7, 22]  \n",
    "df_notes_dataset = df_notes_dataset.drop(df_notes_dataset.columns[columns_to_remove], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_notes_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a .tsv file into a DataFrame\n",
    "file_path = # Replace with your file path to 'Note Status History' dataset \n",
    "df_status_history_dataset = pd.read_csv(file_path, sep='\\\\t')\n",
    "\n",
    "\n",
    "# Remove columns by their indexes\n",
    "columns_to_remove = [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]  # Replace with the indexes of the columns you want to remove\n",
    "df_status_history_dataset = df_status_history_dataset.drop(df_status_history_dataset.columns[columns_to_remove], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_status_history_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a .tsv file into a DataFrame\n",
    "file_path = # Replace with your file path to 'Ratings' dataset\n",
    "df_ratings_dataset = pd.read_csv(file_path, sep='\\\\t')\n",
    "\n",
    "\n",
    "# Remove columns by their indexes\n",
    "columns_to_remove = [1, 3, 6, 7, 9, 10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 25, 26, 27, 28, 29, 30, 31] \n",
    "df_ratings_dataset = df_ratings_dataset.drop(df_ratings_dataset.columns[columns_to_remove], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_ratings_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the intersection of noteId values across all three datasets\n",
    "common_note_ids = set(df_notes_dataset['noteId']) & set(df_status_history_dataset['noteId']) & set(df_ratings_dataset['noteId'])\n",
    "\n",
    "# Filter each dataset to keep only rows with noteId values in the intersection\n",
    "df_notes_dataset = df_notes_dataset[df_notes_dataset['noteId'].isin(common_note_ids)]\n",
    "df_status_history_dataset = df_status_history_dataset[df_status_history_dataset['noteId'].isin(common_note_ids)]\n",
    "df_ratings_dataset = df_ratings_dataset[df_ratings_dataset['noteId'].isin(common_note_ids)]\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(f\"Filtered df_notes_dataset: {df_notes_dataset.shape}\")\n",
    "print(f\"Filtered df_status_history_dataset: {df_status_history_dataset.shape}\")\n",
    "print(f\"Filtered df_ratings_dataset: {df_ratings_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned DataFrames to CSV files\n",
    "\n",
    "df_notes_dataset.to_csv(# File path Notes dataset clean_1, index=False)\n",
    "df_status_history_dataset.to_csv(# File path Note Status History dataset clean_1, index=False)\n",
    "df_ratings_dataset.to_csv(# File path  Ratings dataset clean_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load in the previously cleaned datasets for further processing and analysis as new Dataframes\n",
    "\n",
    "df_notes = pd.read_csv(#input file path to Notes dataset clean_1)\n",
    "df_status_history = pd.read_csv(#input file path to Note Status History dataset clean_1)\n",
    "df_ratings = pd.read_csv(#input file path to Ratings dataset clean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890da7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we filter the datasets to include only entries created in 2024 and later, which is usful for my particular analysis.\n",
    "\n",
    "# Define the timestamp for the start of 2024 (in milliseconds)\n",
    "\n",
    "start_of_2024 = 1704067200000  # January 1, 2024, in milliseconds\n",
    "\n",
    "# Filter df_notes_dataset\n",
    "df_notes['createdAtMillis'] = pd.to_numeric(df_notes['createdAtMillis'], errors='coerce')\n",
    "df_notes = df_notes[df_notes['createdAtMillis'] >= start_of_2024]\n",
    "\n",
    "# Filter df_status_history_dataset\n",
    "df_status_history['createdAtMillis'] = pd.to_numeric(df_status_history ['createdAtMillis'], errors='coerce')\n",
    "df_status_history = df_status_history [df_status_history ['createdAtMillis'] >= start_of_2024]\n",
    "\n",
    "# Filter df_ratings_dataset\n",
    "df_ratings['createdAtMillis'] = pd.to_numeric(df_ratings['createdAtMillis'], errors='coerce')\n",
    "df_ratings = df_ratings[df_ratings['createdAtMillis'] >= start_of_2024]\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(f\"Filtered df_notes_dataset: {df_notes.shape}\")\n",
    "print(f\"Filtered df_status_history_dataset: {df_status_history.shape}\")\n",
    "print(f\"Filtered df_ratings_dataset: {df_ratings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the version 2 of the cleaned DataFrames to CSV files, these include a specific timeframe filter.\n",
    "\n",
    "df_notes.to_csv(# File path Notes dataset clean_2, index=False)\n",
    "df_status_history.to_csv(# File path Note Status History dataset clean_2, index=False)\n",
    "df_ratings.to_csv(# File path  Ratings dataset clean_2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to extract the URLs from the Notes datasets which can be identified with the URLs of European fact-checking organisations. \n",
    "\n",
    "# We use a regular expression to find URLs in the 'summary' column of the Notes dataset and create a new column 'sourceURL' with the extracted URLs.\n",
    "import re\n",
    "\n",
    "df_notes = pd.read_csv(# input file path to Notes dataset clean_2)\n",
    "    \n",
    "# Define a function to extract URLs from a string\n",
    "def extract_urls(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return None\n",
    "    # Regular expression to match URLs\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls[0] if urls else None  # Return the first URL found, or None if no URL is found\n",
    "\n",
    "# Apply the function to the 'summary' column and create a new column 'sourceURL'\n",
    "df_notes['sourceURL'] = df_notes['summary'].apply(extract_urls)\n",
    "\n",
    "# Drop rows where 'sourceURL' is NaN (i.e., rows without a URL)\n",
    "df_notes = df_notes.dropna(subset=['sourceURL'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_notes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use tldextract to extract the domain and suffix from the 'sourceURL' column and add it to a new column 'domainExtract'.\n",
    "\n",
    "import tldextract\n",
    "\n",
    "# Extract the domain and suffix from the sourceURL column and add it to a new column 'domainExtract'\n",
    "df_notes['domainExtract'] = df_notes['sourceURL'].apply(lambda url: f\"{tldextract.extract(url).domain}.{tldextract.extract(url).suffix}\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_notes[['sourceURL', 'domainExtract']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we match the 'domainExtract' column with a predefined list of domains to identify the source of the Notes.\n",
    "\n",
    "# Define the list of domains to match\n",
    "EMDO_members = [\n",
    "    \"afp.com\",\n",
    "    \"correctiv.org\",\n",
    "    \"delfi.lt\",\n",
    "    \"demagog.org.pl\",\n",
    "    \"dpa.com\",\n",
    "    \"verifica.efe.com\",\n",
    "    \"thejournal.ie\",\n",
    "    \"faktabaari.fi\",\n",
    "    \"faktisk.no\",\n",
    "    \"ellinikahoaxes.gr\",\n",
    "    \"kallkritikbyran.se\",\n",
    "    \"knack.be\",\n",
    "    \"lessurligneurs.eu\",\n",
    "    \"maldita.es\",\n",
    "    \"mimikama.org\",\n",
    "    \"newsguardtech.com\",\n",
    "    \"newtral.es\",\n",
    "    \"ostro.si\",\n",
    "    \"pagellapolitica.it\",\n",
    "    \"poligrafo.sapo.pt\",\n",
    "    \"science.feedback.org\",\n",
    "    \"tjekdet.dk\",\n",
    "    \"rtve.es\",\n",
    "    \"verificat.cat\"\n",
    "]\n",
    "\n",
    "EFCSN_members = [\n",
    "    \"hibrid.info\",\n",
    "    \"afp.com\",\n",
    "    \"apa.at\",\n",
    "    \"civilnet.am\",\n",
    "    \"correctiv.org\",\n",
    "    \"delfi.lt\",\n",
    "    \"demagog.org.pl\",\n",
    "    \"demagog.cz\",\n",
    "    \"demagog.sk\",\n",
    "    \"dogrula.org\",\n",
    "    \"dpa.com\",\n",
    "    \"verifica.efe.com\",\n",
    "    \"ellinikahoaxes.gr\",\n",
    "    \"factcheckcyprus.org\",\n",
    "    \"facta.news\",\n",
    "    \"factcheck.ge\",\n",
    "    \"factcheck.bg\",\n",
    "    \"factcheck.vlaanderen\",\n",
    "    \"factcheckni.org\",\n",
    "    \"factreview.gr\",\n",
    "    \"factual.ro\",\n",
    "    \"fakenews.rs\",\n",
    "    \"faktisk.no\",\n",
    "    \"faktograf.hr\",\n",
    "    \"faktoje.al\",\n",
    "    \"fullfact.org\",\n",
    "    \"maldita.es\",\n",
    "    \"geofacts.ee\",\n",
    "    \"factchecker.gr\",\n",
    "    \"info-veritas.com\",\n",
    "    \"kallxo.com\",\n",
    "    \"istinomer.rs\",\n",
    "    \"istinomjer.ba\",\n",
    "    \"kallkritikbyran.se\",\n",
    "    \"knack.be\",\n",
    "    \"lakmusz.hu\",\n",
    "    \"leadstories.com\",\n",
    "    \"lessurligneurs.eu\",\n",
    "    \"lupa.lupiga.com\",\n",
    "    \"medizin-transparent.at\",\n",
    "    \"mythdetector.com\",\n",
    "    \"newtral.es\",\n",
    "    \"ostro.si\",\n",
    "    \"pa.media\",\n",
    "    \"pagellapolitica.it\",\n",
    "    \"15min.lt\",\n",
    "    \"poligrafo.sapo.pt\",\n",
    "    \"pravda.org.pl\",\n",
    "    \"provereno.media\",\n",
    "    \"raskrikavanje.rs\",\n",
    "    \"raskrinkavanje.ba\",\n",
    "    \"raskrinkavanje.me\",\n",
    "    \"rebaltica.lv\",\n",
    "    \"science.feedback.org\",\n",
    "    \"teyit.org\",\n",
    "    \"tjekdet.dk\",\n",
    "    \"verificat.cat\",\n",
    "    \"viral.sapo.pt\",\n",
    "    \"vistinomer.mk\",\n",
    "    \"vrt.be\",\n",
    "    \"investigatebel.org\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Create regex patterns for EFCSN and EDMO members\n",
    "efcsn_pattern = '|'.join(EFCSN_members)\n",
    "edmo_pattern = '|'.join(EMDO_members)\n",
    "\n",
    "# Add columns to indicate whether the domain is part of EFCSN, EDMO, or Other Source\n",
    "df_notes['noteFromEFCSN'] = df_notes['domainExtract'].str.contains(efcsn_pattern, na=False)\n",
    "df_notes['noteFromEDMO'] = df_notes['domainExtract'].str.contains(edmo_pattern, na=False)\n",
    "df_notes['noteFromOtherSource'] = ~(\n",
    "    df_notes['noteFromEFCSN'] | df_notes['noteFromEDMO']\n",
    ")\n",
    "\n",
    "# Convert boolean columns to integers (optional, if needed)\n",
    "df_notes['noteFromEFCSN'] = df_notes['noteFromEFCSN'].astype(int)\n",
    "df_notes['noteFromEDMO'] = df_notes['noteFromEDMO'].astype(int)\n",
    "df_notes['noteFromOtherSource'] = df_notes['noteFromOtherSource'].astype(int)\n",
    "\n",
    "# Count the number of true values in each boolean column\n",
    "true_counts = {\n",
    "    \"noteFromEFCSN\": df_notes['noteFromEFCSN'].sum(),\n",
    "    \"noteFromEDMO\": df_notes['noteFromEDMO'].sum(),\n",
    "    \"noteFromOtherSource\": df_notes['noteFromOtherSource'].sum()\n",
    "}\n",
    "\n",
    "# Display the counts\n",
    "print(true_counts)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_notes.to_csv(# File path Notes dataset clean_3, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e383aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the total number of unique notes that are from either EFCSN or EDMO or both, so we know how many unique notes are from these sources.\n",
    "\n",
    "# Get notes that are from either EFCSN or EDMO or both\n",
    "either_efcsn_or_edmo = df_notes[(df_notes['noteFromEFCSN'] == 1) | (df_notes['noteFromEDMO'] == 1)]\n",
    "\n",
    "# Count unique noteIds\n",
    "unique_note_count = either_efcsn_or_edmo['noteId'].nunique()\n",
    "\n",
    "print(f\"Total number of unique notes from either EFCSN or EDMO members: {unique_note_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13573ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE END OF NOTEBOOK CLEANING AND PROCESSING\n",
    "\n",
    "# THE MANUAL ANNOTATION OF THE DATASETS WILL BE DONE IN NOTEBOOK 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
